---
title: "StatLr-P1-O3"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
```

## Task 3a)
* We want to show that $logit(p_i) = log(\frac{p_i}{1-p_i})$ is a linear function, where $p_i = \frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}$. Thus, we get:
\begin{equation}
log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{1-\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg)= log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{\frac{1}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg) = log \Bigg(exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) \Bigg) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}.
\end{equation}
And we get $log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, which is a linear function.
```{r, echo=FALSE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]

fit = glm(y ~ ., data = train, family = "binomial")
coef(fit)
```
* There is no obivous interpretation of the $\hat{\beta_1}$ and $\hat{\beta_2}$ since a increase of one unit in $\hat{\beta}$ does not give a consistent increase or decrease for all values of $x_1$. It is however easier to look at the odds $\frac{p_i}{1-p_i}$, which is bounded from below, but not above. The interpretation of the odds in this example is the conditional probability that a wine is in class 1, divided by the probability that the wine is in class 0 for the covariates $x_1$ and $x_2$. Moreocer a increase from $x_{1i}$ to $x_{1i} + 1$, the odds are multiplied by $e^{\beta_1}$, which gives us a more intuitive interpretation of $\hat{\beta_1}$. The case is similar for $\hat{\beta_2}$ for a increase in $x_{2i}$, and we can see that $\beta < 0$ the odds will decrease, for $\beta_1 = 0$ the odds will stay the same, and for $\beta_1 >0$, the odds will increase.

* Since the odds is linear (as shown before), we have a linear class boundary. From earlier, we have: $log \bigg(\frac{Pr(Y_i=1|X=x_i)}{Pr(Y_i=0|X=x_i)} \bigg) = log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, where we want to fit the coefficients by maximaizing the likelihood. Then, where $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} = 0$, we have the class boundary.
```{r,echo=FALSE}
a = coef(fit)[1]
b = coef(fit)[2]
c = coef(fit)[3]
g1 = ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=3)+geom_abline(slope=b/(-c),intercept=a/(-c))
g1 = g1 + ggtitle("train and test and logistic boundary")
g1
summary(fit)
```
* Using the summary output, we can get the coefficients $\beta_0$, $\beta_1$ and $\beta_2$. Thus by using the formula:
\begin{equation}
P(Y=1|x_1 = 17, x_2=3) = \frac{exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} = \frac{exp( 7.3143 + 0.1332*17 - 2.3361*3)}{1+exp( 7.3143 + 0.1332*17 - 2.3361*3)} = \frac{exp(2.5704)}{1+exp(2.5704)}=0.9289.
\end{equation}
This result can be interpreted as the probability for a observation to be in class $Y=1$ given the variable values $x_1 = 17$ and $x_2=3$.

```{r}

prob = predict(fit, newdata=test, type="response")
pred = ifelse(prob > 0.5, 1, 0)
t = table (pred, test$y)
t
```
* The sensitivity is the propotion of correctly classified positive observations, and the specificity is the propotion of correctly classed positive observations. Thus, the values lies between 0 and 1, and the goal is to obtain high values. It's important to note that large value for only one of them is not necessarily any good. For example, a method which just puts every observation in class 1 will have sensitivity 1, but specificity 0. Therefore, we want high values (close to one), but balanced as well. There are cases where there is more important that we get the classification of one class right, than the other, for which we can sacrifice the balance to obtain either a high sensitivity or a high spesificity (example would be to avoid a Type 1 error, for which we can sacrifice the probabolity for actually rejecting a fals $H_0$). That being said, in this case there is no worse to classify a 1-wine as a 0-wine, than the opposite.
```{r, echo=FALSE}
sens=t[2,2]/sum(t[2,])
spec=t[1,1]/sum(t[1,])
sens
spec
```
They both obtain relativly large values, and one can therefor argue that the classification works pretty good. Similar values for the sensitivity and the spesificity suggests that the classification does not prefer or default to one class, which in this case as discussed above is 

## Task 3b)
\begin{equation}
Pr(Y=||X=x_0)=\frac{1}{K} \sum_{i \in N_o} I(y_i=j)
\end{equation}
* Given the point $x_0$, the equation finds the K nearest points in the dataset, and count the most frequent class among the neigbhours. The right side of the equation finds the precentage of the neighbours which belong to class j, and based on that, we take a decision whether $x_=$ should be in class j or not. 
```{r, echo=FALSE, warning=FALSE}
#library(class)
#library(dplyr)
#library(ggpubr)
#nn3 = knn(train = train, test = test, cl = train$y, k=3)
#t3 = table(nn3, test$y)
#t3
#sens3=t3[2,2]/sum(t3[2,])
#spec3=t3[1,1]/sum(t3[1,])
#sens3
#spec3
#nn9 = knn(train = train, test = test, cl = train$y, k=9)
#t9 = table(nn9, test$y)
#t9
#sens9=t9[2,2]/sum(t9[2,])
#spec9=t9[1,1]/sum(t9[1,])
#sens9
#spec9

#plot.df = data.frame(test, predicted = nn3)

#plot.df1 = data.frame(x = plot.df$x1,  y = plot.df$x2,  predicted = plot.df$y)


#ggplot(plot.df, aes(x1, x2, color = predicted, fill = predicted)) + geom_point(size = 5)

library(class)
library(dplyr)
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
cbind(KNN3probwinning,KNN3prob)
testclass3=ifelse(KNN3prob>0.5,1,0)
t3 = table(test$y,testclass3)
t3
sens3=t3[2,2]/sum(t3[2,])
spec3=t3[1,1]/sum(t3[1,])
sens3
spec3
KNN9 = knn(train = train[,-1], test = test[,-1], cl = train$y, k=9, prob=F)
KNN9probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 9, cl = train$y, prob = TRUE))$prob
KNN9prob <- ifelse(KNN9 == "0", 1-KNN9probwinning, KNN9probwinning)
cbind(KNN9probwinning,KNN9prob)
testclass9=ifelse(KNN9prob>0.5,1,0)
t9 = table(test$y, testclass9)
t9
sens9=t9[2,2]/sum(t9[2,])
spec9=t9[1,1]/sum(t9[1,])
sens9
spec9

```
As we can see from the data, the sencitivity and specificity for k=3 is higher than for k=9. That happens because the neighbours in k=9 becomes further away, and the method doesn't become local anymore.