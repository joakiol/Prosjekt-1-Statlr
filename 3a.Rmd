---
title: "StatLr-P1-O3"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Oppgave 3A)
* We want to show that $logit(p_i) = log(\frac{p_i}{1-p_i})$ is a linear function, where $p_i = \frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}$. Thus, we get:
\begin{equation}
log \Bigg(\frac{\frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}}{1-\frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}} \Bigg)= log \Bigg(\frac{\frac{e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}}{\frac{1}{1+e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}}} \Bigg) = log \Bigg(e^{\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}} \Bigg) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}.
\end{equation}
And we get $log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, which is a linear function.
```{r first, echo=FALSE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]

fit = glm(y ~ ., data = train, family = "binomial")
coef(fit)
```
* There is no obivous interpretation of the $\hat \beta_1$ and $\hat \beta_2$ since a increase of one unit in $\hat \beta$ does not give a consistent increase or decrease for all values of $x_1$. It is however easier to look at the odds $\frac{p_i}{1-p_i}$, which is bounded from below, but not above. The interpretation of the odds in this example is the conditional probability that a wine is in class 1, divided by the probability that the wine is in class 0 for the covariates $x_1$ and $x_2$. Moreocer a increase from $x_{1i}$ to $x_{1i} + 1$, the odds are multiplied by $exp(\beta_1)$, which gives us a more intuitive interpretation of $\hat \beta_1$. The case is similar for $\hat \beta_2$ for a increase in $x_{2i}$, and we can see that $\beta < 0$ the odds will decrease, for $\beta_1 = 0$ the odds will stay the same, and for $\beta_1 >0 $, the odds will increase.

* Since the odds is linear (as shown before), we have a linear class boundary. From earlier, we have: $log \bigg(\frac{Pr(Y_i=1|X=x_i)}{Pr(Y_i=0|X=x_i)} \bigg) = log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, where we want to fit the coefficients by maximaizing the likelihood. Then, where $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} = 0$, we have the class boundary.
```{r plot,echo=FALSE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]
fit = glm(y ~ ., data = train, family = "binomial")
a = coef(fit)[1]
b = coef(fit)[2]
c = coef(fit)[3]
ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=3)+geom_abline(slope=b/(-c),intercept=a/(-c))
summary(fit)
```