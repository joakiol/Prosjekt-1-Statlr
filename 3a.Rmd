---
title: "StatLr-P1-O3"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 3a)
* We want to show that $logit(p_i) = log(\frac{p_i}{1-p_i})$ is a linear function, where $p_i = \frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}$. Thus, we get:
\begin{equation}
log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{1-\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg)= log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{\frac{1}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg) = log \Bigg(exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) \Bigg) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}.
\end{equation}
And we get $log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, which is a linear function.
```{r, echo=FALSE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle 
test = wine[ord[1:(n/2)],]
train = wine[ord[((n/2)+1):n],]

fit = glm(y ~ ., data = train, family = "binomial")
coef(fit)
```
* There is no obivous interpretation of the $\hat \beta_1$ and $\hat \beta_2$ since a increase of one unit in $\hat \beta$ does not give a consistent increase or decrease for all values of $x_1$. It is however easier to look at the odds $\frac{p_i}{1-p_i}$, which is bounded from below, but not above. The interpretation of the odds in this example is the conditional probability that a wine is in class 1, divided by the probability that the wine is in class 0 for the covariates $x_1$ and $x_2$. Moreocer a increase from $x_{1i}$ to $x_{1i} + 1$, the odds are multiplied by $exp(\beta_1)$, which gives us a more intuitive interpretation of $\hat \beta_1$. The case is similar for $\hat \beta_2$ for a increase in $x_{2i}$, and we can see that $\beta < 0$ the odds will decrease, for $\beta_1 = 0$ the odds will stay the same, and for $\beta_1 >0 $, the odds will increase.

* Since the odds is linear (as shown before), we have a linear class boundary. From earlier, we have: $log \bigg(\frac{Pr(Y_i=1|X=x_i)}{Pr(Y_i=0|X=x_i)} \bigg) = log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, where we want to fit the coefficients by maximaizing the likelihood. Then, where $\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} = 0$, we have the class boundary.
```{r,echo=FALSE}
a = coef(fit)[1]
b = coef(fit)[2]
c = coef(fit)[3]
g1 = ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=3)+geom_abline(slope=b/(-c),intercept=a/(-c))
g1 = g1 + ggtitle("train and test and logistic boundary")
summary(fit)
```
*Using the summary output, we can get the coefficients $\beta_0$, $\beta_1$ and $\beta_2$. Thus by using the formula:
\begin{equation}
P(Y=1|x_1 = 17, x_2=3) = \frac{exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} = \frac{exp( 7.3143 + 0.1332*17 - 2.3361*3)}{1+exp( 7.3143 + 0.1332*17 - 2.3361*3)} = \frac{exp(2.5704)}{1+exp(2.5704)}=0.9289.
\end{equation}
This result can be interpreted as the probability for a observation to be in class $Y=1$ given the variable values $x_1 = 17$ and $x_2=3$.

```{r}

prob = predict(fit, newdata=test, type="response")
pred = ifelse(prob > 0.5, 1, 0)
t = table (pred, test$y)
t
```

```{r, echo=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)

row1 = c("", "Predicted -", "Predicted +","Total")
row2 = c("True -", "True Negative TN", "False Positive FP","N")
row3 = c("True +", "False Negative FN", "True Positive TP","P")
row4 = c("Total", "N*", "P*"," ")
kable(rbind(row1, row2, row3,row4), row.names=FALSE)

```
The sensitivity is the propotion of correctly classified positive observations, and the specificity is the propotion of correctly classed positive observations. 
```{r, echo=FALSE, warning=FALSE}
sens=t[2,2]/sum(t[2,])
spec=t[1,1]/sum(t[1,])
sens
spec
```
They both obtain relativly large values, and one can therefor argue that the classification works pretty good. Similar values for the sensitivity and the spesificity suggests that the classification does not prefer or default to one class, which is quite valuable.