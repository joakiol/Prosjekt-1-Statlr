---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group X"
author: "NN1, NN and NN3"
date: "Date when you hand in"
output: #3rd letter intentation hierarchy
  pdf_document:
  #   theme: tactile
  #   highlight: github
  # prettydoc::html_pretty:
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Take a look at the cheat sheets for R Markdown here: File > Help > Cheatsheets > R Markdown Cheat Sheet in RStudio, or here
<http://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf>

or the lessons: <http://rmarkdown.rstudio.com/lesson-1.html>

# Problem 1 - Core concepts in statistical learning [2 points]

## a) Training and test MSE [1 point]
Here you write your smart answer. You may write latex $Y=f(x)+\varepsilon$.

Items with

* item1
* item2

## b) Bias-variance trade-off [1 point]

* Explain how that is done. Hint: this is what the $M$ repeated training data sets are used for. 
* Focus on Figure 4. As the flexibility of the model increases ($K$ decreases), what happens with
    + the squared bias,  
    + the variance, and  
    + the irreducible error?
* What would you recommend is the optimal value of $K$? Is this in agreement with what you found in a)?

# Problem 2 - Linear regression [4 points]

Here you see an R chunk that is evaluated (when knitting) and code is displayed.

```{r,echo=TRUE,eval=FALSE}
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
dim(data)
colnames(data)
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
summary(modelA)
```

## a) Understanding model output [1 point]
## b) Model fit [1 point]
## c) Confidence interval and hypothesis test [1 points]
## d) Prediction [1 point]

#Problem 3 - Classification

## c) LDA (& QDA)
 The expression from the task is found by assuming the classes are normal distributed according to the function  


$$f_k(x)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_k})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_k})}.$$ 
The posterior probability is the calculated as 
$$p_k(x)=Pr(Y=k|\boldsymbol{X}=\boldsymbol{x})=\frac{Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)Pr(Y=k)}{Pr(\boldsymbol{X}=\boldsymbol{x})}=\frac{\pi_kf_k(\boldsymbol{x})}{\sum_{l=1}^K\pi_lf_l(\boldsymbol{x})}$$

* Here, $\pi_k=Pr(Y=k)$, which is the probability that a given wine sample $Y$ belongs to class $k$. $\boldsymbol{\mu_k}$ is the vector containing the expected values of the parametres, which is color intensity and alcalinity of ash in this task. $\boldsymbol{\Sigma}$ is the covariance matrix for the parametres, which contains the variance of the parametres and the covariance between them. $f_k(x)$ is as mentioned the probability density function for a given class, which again is the conditional probability of $x$ with a given class, $f_k(x)=Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)$. 

* To estimate $\pi_k$ we can use the the total number of samples in the training set $n$, and the number of samples belonging to the requested class, $n_k$. We can then use te estimator $\hat{\pi}_k=\frac{n_k}{n}$. $\mu_k$ can be estimated simply using a sample mean from all the samples belonging to the requested class. The estimator is then calculated as $\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i$. To estimate the covariance matrix, the class-specific covariance matrices can be calculated as
$$\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)^T$$
The true covariance matrix is the found by summing over the difference classes using wheighs according to the fraction of samples in the belonging class. We then get 
$$\hat{\boldsymbol{\Sigma}}=\sum_{k=1}^K \frac{n_k-1}{n-K}\cdot \hat{\boldsymbol{\Sigma}}_k.$$ 
Calculations for the training data is done in R, resulting in values $\pi_0=0.45$, $\pi_1=0.55$, $\boldsymbol{\mu_0}=(17.38 , 5.57)^T$, $\boldsymbol{\mu_1}=(21, 2.97)^T$ and 
$$
\hat{\boldsymbol{\Sigma}}=
\left(\begin{array}{cc}
7.84 & -0.16\\
-0.16 & 1.15
\end{array}\right)
$$


```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
library(plyr)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],] 
train = wine[ord[((n/2)+1):n],]
frequency=count(train, "y")
pi0=(frequency[1, 2]/(frequency[1,2]+frequency[2,2]))
pi1=(frequency[2, 2]/(frequency[1,2]+frequency[2,2]))
mean0=c(mean(train[train$y==0, "x1"]), mean(train[train$y==0, "x2"]))
mean1=c(mean(train[train$y==1, "x1"]), mean(train[train$y==1, "x2"]))
cov0=matrix(0,2,2)
cov1=matrix(0,2,2)
for (i in 1:(n/2)){
  if (train[i, 1]==0){
    x=c(train[i, 2], train[i, 3])
    cov0 = cov0 + (x-mean0) %*% t(x-mean0)
  }else{
    x=c(train[i, 2], train[i, 3])
    cov1 = cov1 + (x-mean1) %*% t(x-mean1)
  }
}
covmatrix=(1/(frequency[1,2]+frequency[2,2]-2))*(cov0+cov1)

```

* We want to find when 
$$Pr(Y=0|\boldsymbol{X}=\boldsymbol{x})=Pr(Y=1|\boldsymbol{X}=\boldsymbol{x})$$
We rewrite this as 
$$\frac{\pi_0 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_0(\boldsymbol{x})}=\frac{\pi_1 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_0(\boldsymbol{x})}$$
Since the denominators are equal on both sides, we can disregard them. By taking logarithms on both sides we achieve 
$$-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0}) + \log{\pi_0}=-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1}) + \log{\pi_1}$$
We now complete the miltiplications, and remove the equal parts on each side to get:
$$\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} +\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}) + \log{\pi_0}=\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} +\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}) + \log{\pi_1}$$
Now we know the covariance matrix is symmetric positive definite. We therefore know $\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} =\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x}$. We therefore finally get 
$$\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x}),$$
where 
$$\delta_k(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k}- \frac{1}{2}\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} + \log{\pi_k}; \quad k\in \{0, 1\}$$. 

* In the following figure, the LDA has been done using the lda-function in R. The plots include the training data together with the classification areas. 

```{r,echo=FALSE, eval=TRUE}
library(class)
library(MASS)
library(ggplot2)
library(dplyr)
wine0_plot = ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=2.5)
testgrid = expand.grid(x1 = seq(min(train[,2]-0.2), max(train[,2]+0.2), 
              by=0.2), x2 = seq(min(train[,3]-0.2), max(train[,3]+0.2), 
              by=0.1))
wine_lda = lda(y~x1+x2, data=train, prior=c(pi0,pi1))
res = predict(object = wine_lda, newdata = testgrid)
type_lda = res$class
postprobs=res$posterior
wine_lda_df = bind_rows(mutate(testgrid, type_lda))
#wine_lda_df$type_lda = as.factor(wine_lda_df$type_lda)

winelda_plot = wine0_plot + geom_point(aes(x = x1, y=x2, colour=type_lda), data=wine_lda_df, size=0.8)
winelda_plot
```

* To find the class boundary formula, we need to solve the equation $\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x})$. We get 

$$\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}- \frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} + \log{\pi_0}=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}- \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\pi_1}  \\
\boldsymbol{x}^T(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} - \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\frac{\pi_1}{\pi_0}}$$

We use estimators for the parametres as discussed earlier in this task. The necessary values are calculated in R, and we get $\log{\frac{\pi_1}{\pi_0}}=0.216$, $\frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}=-41.1$, $\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}=-49.2$ and $(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=(-0.17, 2.63)^T$.
We then get the equation 
$$2.63x_2-0.17x_1=-7.88 \implies \\ x_2 = 0.065x_1 + 3.16$$

```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(dplyr)
inverse=solve(covmatrix)
sum1=log(pi1/pi0)
sum2=(1/2)*t(mean1)%*%solve(covmatrix)%*%mean1
sum3=(1/2)*t(mean0)%*%solve(covmatrix)%*%mean0
sum4=solve(covmatrix)%*%mean1
sum5=solve(covmatrix)%*%mean0
lda_manual=ggplot(test, aes(x=x1, y=x2, color=y))+geom_point(size=2.5) + geom_abline(slope=0.065, intercept=3.16)
lda_manual
confusionmatrix=matrix(0,2,2)
for (i in 1:(n/2)){
  if (test[i,1]==0){
    if (test[i,3]>=(0.065*test[i,2]+3.16)){
      confusionmatrix[1,1]=confusionmatrix[1,1]+1
    }else{
      confusionmatrix[1,2]=confusionmatrix[1,2]+1
    }
  }else{
    if (test[i,3]<=(0.065*test[i,2]+3.16)){
      confusionmatrix[2,2]=confusionmatrix[2,2]+1
    }else{
      confusionmatrix[2,1]=confusionmatrix[2,1]+1
    }
  }
}

```

* By checking the points in the training set, a confusion matrix have been created. We got 
\begin{center}
  \begin{tabular}{ | l | c | r |}
    \hline
     & Predicted 1 & Predicted 2 & Total\\ \hline
    True - & 24 & 6 & 30\\ \hline
    True + & 5 & 30 & 35\\ \hline
    Total & 29 & 36 & \\
    \hline
  \end{tabular}
\end{center}
The sensitivity is then given by 
$$\frac{\text{#True Positive}}{\text{#Condition Positive}}=\frac{24}{30}=0.8$$, while the specificity is given by 
$$\frac{\text{#True Negative}}{\text{#Condition Negative}}=\frac{30}{35}=0.86$$. 