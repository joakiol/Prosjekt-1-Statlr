---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group X"
author: "NN1, NN and NN3"
date: "Date when you hand in"
output: #3rd letter intentation hierarchy
  pdf_document:
  #   theme: tactile
  #   highlight: github
  # prettydoc::html_pretty:
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
```

Take a look at the cheat sheets for R Markdown here: File > Help > Cheatsheets > R Markdown Cheat Sheet in RStudio, or here
<http://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf>

or the lessons: <http://rmarkdown.rstudio.com/lesson-1.html>

# Problem 1 - Core concepts in statistical learning [2 points]

## a) Training and test MSE [1 point]
Here you write your smart answer. You may write latex $Y=f(x)+\varepsilon$.

Items with

* item1
* item2

## b) Bias-variance trade-off [1 point]

* Explain how that is done. Hint: this is what the $M$ repeated training data sets are used for. 
* Focus on Figure 4. As the flexibility of the model increases ($K$ decreases), what happens with
    + the squared bias,  
    + the variance, and  
    + the irreducible error?
* What would you recommend is the optimal value of $K$? Is this in agreement with what you found in a)?

# Problem 2 - Linear regression [4 points]

Here you see an R chunk that is evaluated (when knitting) and code is displayed.

```{r,echo=TRUE,eval=FALSE}
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
dim(data)
colnames(data)
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
summary(modelA)
```

## a) Understanding model output [1 point]
## b) Model fit [1 point]
## c) Confidence interval and hypothesis test [1 points]
## d) Prediction [1 point]

#Problem 3 - Classification

## c) LDA (& QDA)
 The expression from the task is found by assuming the classes are normal distributed according to the function  


$$f_k(x)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_k})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_k})}.$$ 
The posterior probability is the calculated as 
$$p_k(x)=Pr(Y=k|\boldsymbol{X}=\boldsymbol{x})=\frac{Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)Pr(Y=k)}{Pr(\boldsymbol{X}=\boldsymbol{x})}=\frac{\pi_kf_k(\boldsymbol{x})}{\sum_{l=1}^K\pi_lf_l(\boldsymbol{x})}$$

* Here, $\pi_k=Pr(Y=k)$, which is the probability that a given wine sample $Y$ belongs to class $k$. $\boldsymbol{\mu_k}$ is the vector containing the expected values of the parametres, which is color intensity and alcalinity of ash in this task. $\boldsymbol{\Sigma}$ is the covariance matrix for the parametres, which contains the variance of the parametres and the covariance between them. $f_k(x)$ is as mentioned the probability density function for a given class, which again is the conditional probability of $x$ with a given class, $f_k(x)=Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)$. 

* To estimate $\pi_k$ we can use the the total number of samples in the training set $n$, and the number of samples belonging to the requested class, $n_k$. We can then use te estimator $\hat{\pi}_k=\frac{n_k}{n}$. $\mu_k$ can be estimated simply using a sample mean from all the samples belonging to the requested class. The estimator is then calculated as $\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i$. To estimate the covariance matrix, the class-specific covariance matrices can be calculated as
$$\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)^T$$
The true covariance matrix is the found by summing over the difference classes using wheighs according to the fraction of samples in the belonging class. We then get 
$$\hat{\boldsymbol{\Sigma}}=\sum_{k=1}^K \frac{n_k-1}{n-K}\cdot \hat{\boldsymbol{\Sigma}}_k.$$ 
Calculations for the training data is done in R, resulting in values $\pi_0=0.45$, $\pi_1=0.55$, $\boldsymbol{\mu_0}=(17.26 , 5.58)^T$, $\boldsymbol{\mu_1}=(20.18, 2.97)^T$ and 
$$
\hat{\boldsymbol{\Sigma}}=
\left(\begin{array}{cc}
7.23 & -0.66\\
-0.66 & 0.95
\end{array}\right)
$$


```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
library(plyr)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],] 
train = wine[ord[((n/2)+1):n],]
cov0=var(train[train$y==0, 2:3])
cov1=var(train[train$y==1, 2:3])

frequency=count(train, "y")
pi0=(frequency[1, 2]/(frequency[1,2]+frequency[2,2]))
pi1=(frequency[2, 2]/(frequency[1,2]+frequency[2,2]))
mean0=c(mean(train[train$y==0, "x1"]), mean(train[train$y==0, "x2"]))
mean1=c(mean(train[train$y==1, "x1"]), mean(train[train$y==1, "x2"]))
covmatrix=(1/(frequency[1,2]+frequency[2,2]-2))*(cov0*(frequency[1,2]-1)+cov1*(frequency[2,2]-1))

```

* We want to find when 
$$Pr(Y=0|\boldsymbol{X}=\boldsymbol{x})=Pr(Y=1|\boldsymbol{X}=\boldsymbol{x})$$
We rewrite this as 
$$\frac{\pi_0 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_l(\boldsymbol{x})}=\frac{\pi_1 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_l(\boldsymbol{x})}$$
Since the denominators are equal on both sides, we can disregard them. By taking logarithms on both sides we achieve
$$-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0}) + \log{\pi_0}=-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1}) + \log{\pi_1}$$
We now complete the miltiplications, and remove the equal parts on each side to get:
$$\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} +\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}) + \log{\pi_0}=\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} +\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}) + \log{\pi_1}$$
Now we know the covariance matrix is symmetric positive definite. We therefore know $\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} =\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x}$. We therefore finally get 
$$\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x}),$$
where 
$$\delta_k(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k}- \frac{1}{2}\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} + \log{\pi_k}; \quad k\in \{0, 1\}$$. 

<!-- * In the following figure, the LDA has been done using the lda-function in R. The plots include the training data together with the classification areas.  -->

<!-- ```{r,echo=FALSE, eval=TRUE} -->
<!-- library(class) -->
<!-- library(MASS) -->
<!-- library(ggplot2) -->
<!-- library(dplyr) -->
<!-- wine0_plot = ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=2.5) -->
<!-- #testgrid = expand.grid(x1 = seq(min(train[,2]-0.2), max(train[,2]+0.2),  -->
<!-- #              by=0.2), x2 = seq(min(train[,3]-0.2), max(train[,3]+0.2),  -->
<!-- #              by=0.1)) -->
<!-- #wine_lda = lda(y~x1+x2, data=train, prior=c(pi0,pi1)) -->
<!-- wine_lda = lda(y~x1+x2, data=train) -->
<!-- wine_lda -->
<!-- #res = predict(object = wine_lda, newdata = testgrid) -->
<!-- #type_lda = res$class -->
<!-- #postprobs=res$posterior -->
<!-- #wine_lda_df = bind_rows(mutate(testgrid, type_lda)) -->
<!-- #wine_lda_df$type_lda = as.factor(wine_lda_df$type_lda) -->

<!-- #winelda_plot = wine0_plot + geom_point(aes(x = x1, y=x2, colour=type_lda), data=wine_lda_df, size=0.8) -->
<!-- winelda_plot = wine0_plot + wine_lda -->
<!-- #winelda_plot -->
<!-- ``` -->

* To find the class boundary formula, we need to solve the equation $\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x})$. We get 
$$\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}- \frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} + \log{\pi_0}=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}- \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\pi_1}$$
$$\boldsymbol{x}^T(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} - \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\frac{\pi_1}{\pi_0}}$$
We use estimators for the parametres as discussed earlier in this task. The necessary values are calculated in R, and we get $\log{\frac{\pi_1}{\pi_0}}=0.216$, $\frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}=-41.1$, $\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}=-49.2$ and $(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=(-0.17, 2.63)^T$.
We then get the equation 
$$2.628x_2-0.164x_1=8.368 \implies \\ x_2 = 0.0625x_1 + 3.184$$

* The following plot shows the training data as circular dots, together with the test data as crosses. The plot also shows the line representing equal probability of being in the two classes, based on the training data using LDA. 

```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(dplyr)
inverse=solve(covmatrix)
sum1=log(pi1/pi0)
sum2=(1/2)*t(mean1)%*%solve(covmatrix)%*%mean1
sum3=(1/2)*t(mean0)%*%solve(covmatrix)%*%mean0
sum4=solve(covmatrix)%*%mean1
sum5=solve(covmatrix)%*%mean0
leftside=sum5-sum4
rightside=sum1+sum3-sum2
slope1=-leftside[1]/leftside[2]
intercept1=rightside/leftside[2]
lda_manual=ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=2.5) + geom_point(data=test, shape=4)+ geom_abline(slope=slope1, intercept=intercept1)
lda_manual

```

* LDA is also performed using the built in R-function. This is done in the R-code shown below. We see the data corresponds to the estimators we have used earlier, which is natural as the estimators are calculated the same way. 

```{r,echo=TRUE, eval=TRUE}
lda_train=lda(y~x1+x2, data=train)
lda_train

```



\newpage



```{r,echo=FALSE, eval=TRUE}
library(knitr)
#library(kableExtra)
lpred=predict(lda_train, newdata=test)
confusiontable=table(test$y, lpred$class)
row1 = c("", "Predicted -", "Predicted +","Total")
row2 = c("True -", 24, 6,30)
row3 = c("True +", 5, 30,35)
row4 = c("Total", 29, 36," ")
kable(rbind(row1, row2, row3,row4), row.names=FALSE)
```
* The table above shows the confusion table for the test data, based on the lda from the training data.
The sensitivity is then given by 
$$\frac{\text{True Positive}}{\text{Condition Positive}}=\frac{24}{30}=0.8,$$ 
while the specificity is given by
$$\frac{\text{True Negative}}{\text{Condition Negative}}=\frac{30}{35}=0.86.$$
We want high specifity and seinsitivity, and we see values are fairly close to 1, which mean we get many correct predictions. It still seems like we will do mistakes every now and then, as there are a considerable number of mistakes. 

* In QDA, the covariance matrices for the different classes are allowed to be different. This makes this method more complex, but also more flexible. The decision boundaries will now become quadratic functions of $\boldsymbol{x}$. 

## d) Compare classifiers

* For logistic regression we got specificity 0.833 and sensitivity 0.857 . For KNN with our preferred $K=3$, we got specificity 0.964  and sensitivity 0.919. For lda we got 0.800 and specificity 0.857. Since we want high specificity and sensitivity, our preferred method when using 0.5 as cut-off is KNN. This method performs considerably better than the other two methods, which are more equal, though logistic regression seems to be slightly better. ha med dette?:Note however that the training and test sets are not that big, and that might influence which method performs better. Our result is still an indication that a strictly linear model might not be optimal for the wine classification problem. 

* 


```{r,echo=FALSE, eval=TRUE}

glmroc=roc(response=test$y,predictor=prob)
plot(glmroc)
auc(glmroc)
KNN3 = knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = F)
KNN3probwinning = attributes(knn(train = train[,-1], test = test[,-1], k = 3, cl = train$y, prob = TRUE))$prob
KNN3prob <- ifelse(KNN3 == "0", 1-KNN3probwinning, KNN3probwinning)
KNN3roc=roc(response=test$y,predictor=KNN3prob)
plot(KNN3roc)
auc(KNN3roc)
ltrain=lda(y~x1+x2,data=train)
lpred=predict(object = ltrain, newdata = test)$posterior[,1]
lroc=roc(response=test$y,lpred)
plot(lroc)
auc(lroc)
```