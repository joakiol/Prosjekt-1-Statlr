---
subtitle: "TMA4268 Statistical Learning V2018"
title: "Compulsory exercise 1: Group X"
author: "NN1, NN and NN3"
date: "Date when you hand in"
output: #3rd letter intentation hierarchy
  pdf_document:
  #   theme: tactile
  #   highlight: github
  # prettydoc::html_pretty:
editor_options: 
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warnings=FALSE)
knitr::opts_chunk$set(message =FALSE)
```

Take a look at the cheat sheets for R Markdown here: File > Help > Cheatsheets > R Markdown Cheat Sheet in RStudio, or here
<http://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf>

or the lessons: <http://rmarkdown.rstudio.com/lesson-1.html>

# Problem 1 - Core concepts in statistical learning [2 points]

## a) Training and test MSE [1 point]
Here you write your smart answer. You may write latex $Y=f(x)+\varepsilon$.

Items with

* item1
* item2

## b) Bias-variance trade-off [1 point]

* Explain how that is done. Hint: this is what the $M$ repeated training data sets are used for. 
* Focus on Figure 4. As the flexibility of the model increases ($K$ decreases), what happens with
    + the squared bias,  
    + the variance, and  
    + the irreducible error?
* What would you recommend is the optimal value of $K$? Is this in agreement with what you found in a)?

# Problem 2 - Linear regression [4 points]

Here you see an R chunk that is evaluated (when knitting) and code is displayed.

```{r,echo=TRUE,eval=FALSE}
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
dim(data)
colnames(data)
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
summary(modelA)
```

## a) Understanding model output [1 point]
## b) Model fit [1 point]
## c) Confidence interval and hypothesis test [1 points]
## d) Prediction [1 point]

#Problem 3 - Classification

## c) LDA (& QDA)
 The expression from the task is found by assuming the classes are normal distributed according to the function  


$$f_k(x)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_k})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_k})}.$$ 
The posterior probability is the calculated as 
$$p_k(x)=Pr(Y=k|\boldsymbol{X}=\boldsymbol{x})=\frac{Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)Pr(Y=k)}{Pr(\boldsymbol{X}=\boldsymbol{x})}=\frac{\pi_kf_k(\boldsymbol{x})}{\sum_{l=1}^K\pi_lf_l(\boldsymbol{x})}$$

* Here, $\pi_k=Pr(Y=k)$, which is the probability that a given wine sample $Y$ belongs to class $k$. $\boldsymbol{\mu_k}$ is the vector containing the expected values of the parametres, which is color intensity and alcalinity of ash in this task. $\boldsymbol{\Sigma}$ is the covariance matrix for the parametres, which contains the variance of the parametres and the covariance between them. $f_k(x)$ is as mentioned the probability density function for a given class, which again is the conditional probability of $x$ with a given class, $f_k(x)=Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)$. 

* To estimate $\pi_k$ we can use the the total number of samples in the training set $n$, and the number of samples belonging to the requested class, $n_k$. We can then use te estimator $\hat{\pi}_k=\frac{n_k}{n}$. $\mu_k$ can be estimated simply using a sample mean from all the samples belonging to the requested class. The estimator is then calculated as $\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i$. To estimate the covariance matrix, the class-specific covariance matrices can be calculated as
$$\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)^T$$
The true covariance matrix is the found by summing over the difference classes using wheighs according to the fraction of samples in the belonging class. We then get 
$$\hat{\boldsymbol{\Sigma}}=\sum_{k=1}^K \frac{n_k-1}{n-K}\cdot \hat{\boldsymbol{\Sigma}}_k.$$ 
Calculations for the training data is done in R, resulting in values $\pi_0=0.45$, $\pi_1=0.55$, $\boldsymbol{\mu_0}=(17.38 , 5.57)^T$, $\boldsymbol{\mu_1}=(21, 2.97)^T$ and 
$$
\hat{\boldsymbol{\Sigma}}=
\left(\begin{array}{cc}
7.84 & -0.16\\
-0.16 & 1.15
\end{array}\right)
$$


```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(GGally)
library(class)
library(MASS)
library(pROC)
library(plyr)
wine=read.csv("https://www.math.ntnu.no/emner/TMA4268/2018v/data/Comp1Wine.csv",sep=" ")
wine$class=as.factor(wine$class-1)
colnames(wine)=c("y","x1","x2")
#ggpairs(wine, ggplot2::aes(color=y))
n=dim(wine)[1]
set.seed(4268) #to get the same order if you rerun - but you change this to your favorite number
ord = sample(1:n) #shuffle
test = wine[ord[1:(n/2)],] 
train = wine[ord[((n/2)+1):n],]
train0=train[train$y==0, 2:3]
train1=train[train$y==1, 2:3]

frequency=count(train, "y")
pi0=(frequency[1, 2]/(frequency[1,2]+frequency[2,2]))
pi1=(frequency[2, 2]/(frequency[1,2]+frequency[2,2]))
mean0=c(mean(train[train$y==0, "x1"]), mean(train[train$y==0, "x2"]))
mean1=c(mean(train[train$y==1, "x1"]), mean(train[train$y==1, "x2"]))
cov0=matrix(0,2,2)
cov1=matrix(0,2,2)
for (i in 1:(n/2)){
  if (train[i, 1]==0){
    x=c(train[i, 2], train[i, 3])
    cov0 = cov0 + (x-mean0) %*% t(x-mean0)
  }else{
    x=c(train[i, 2], train[i, 3])
    cov1 = cov1 + (x-mean1) %*% t(x-mean1)
  }
}
covmatrix=(1/(frequency[1,2]+frequency[2,2]-2))*(cov0+cov1)

```

* We want to find when 
$$Pr(Y=0|\boldsymbol{X}=\boldsymbol{x})=Pr(Y=1|\boldsymbol{X}=\boldsymbol{x})$$
We rewrite this as 
$$\frac{\pi_0 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_0(\boldsymbol{x})}=\frac{\pi_1 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_0(\boldsymbol{x})}$$
Since the denominators are equal on both sides, we can disregard them. By taking logarithms on both sides we achieve 
$$-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0}) + \log{\pi_0}=-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1}) + \log{\pi_1}$$
We now complete the miltiplications, and remove the equal parts on each side to get:
$$\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} +\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}) + \log{\pi_0}=\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} +\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}) + \log{\pi_1}$$
Now we know the covariance matrix is symmetric positive definite. We therefore know $\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} =\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x}$. We therefore finally get 
$$\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x}),$$
where 
$$\delta_k(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k}- \frac{1}{2}\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} + \log{\pi_k}; \quad k\in \{0, 1\}$$. 

* In the following figure, the LDA has been done using the lda-function in R. The plots include the training data together with the classification areas. 

```{r,echo=FALSE, eval=TRUE}
library(class)
library(MASS)
library(ggplot2)
library(dplyr)
wine0_plot = ggplot(train, aes(x=x1, y=x2, color=y))+geom_point(size=2.5)
testgrid = expand.grid(x1 = seq(min(train[,2]-0.2), max(train[,2]+0.2), 
              by=0.2), x2 = seq(min(train[,3]-0.2), max(train[,3]+0.2), 
              by=0.1))
wine_lda = lda(y~x1+x2, data=train, prior=c(pi0,pi1))
res = predict(object = wine_lda, newdata = testgrid)
type_lda = res$class
postprobs=res$posterior
wine_lda_df = bind_rows(mutate(testgrid, type_lda))
#wine_lda_df$type_lda = as.factor(wine_lda_df$type_lda)

winelda_plot = wine0_plot + geom_point(aes(x = x1, y=x2, colour=type_lda), data=wine_lda_df, size=0.8)
winelda_plot
```

* To find the class boundary formula, we need to solve the equation $\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x})$. We get 

$$\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}- \frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} + \log{\pi_0}=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}- \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\pi_1}$$
$$\boldsymbol{x}^T(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} - \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\frac{\pi_1}{\pi_0}}$$

We use estimators for the parametres as discussed earlier in this task. The necessary values are calculated in R, and we get $\log{\frac{\pi_1}{\pi_0}}=0.216$, $\frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}=-41.1$, $\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}=-49.2$ and $(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=(-0.17, 2.63)^T$.
We then get the equation 
$$2.63x_2-0.17x_1=-7.88 \implies \\ x_2 = 0.065x_1 + 3.16$$

```{r,echo=FALSE, eval=TRUE}
library(ggplot2)
library(dplyr)
inverse=solve(covmatrix)
sum1=log(pi1/pi0)
sum2=(1/2)*t(mean1)%*%solve(covmatrix)%*%mean1
sum3=(1/2)*t(mean0)%*%solve(covmatrix)%*%mean0
sum4=solve(covmatrix)%*%mean1
sum5=solve(covmatrix)%*%mean0
lda_manual=ggplot(test, aes(x=x1, y=x2, color=y))+geom_point(size=2.5) + geom_abline(slope=0.065, intercept=3.16)
lda_manual
confusionmatrix=matrix(0,2,2)
for (i in 1:(n/2)){
  if (test[i,1]==0){
    if (train[i,3]>=(0.065*test[i,2]+3.16)){#0.0625, 3.184
      confusionmatrix[1,1]=confusionmatrix[1,1]+1
    }else{
      confusionmatrix[1,2]=confusionmatrix[1,2]+1
    }
  }else{
    if (test[i,3]<=(0.065*test[i,2]+3.16)){
      confusionmatrix[2,2]=confusionmatrix[2,2]+1
    }else{
      confusionmatrix[2,1]=confusionmatrix[2,1]+1
    }
  }
}

```
<!-- * By checking the points in the training set, a confusion matrix have been created. We got  -->
<!-- \begin{center} -->
<!--   \begin{tabular}{ | l | c | r |} -->
<!--     \hline -->
<!--       & Predicted 1 & Predicted 2 & Total\\ \hline -->
<!--     True - & 24 & 6 & 30\\ \hline -->
<!--     True + & 5 & 30 & 35\\ \hline -->
<!--     Total & 29 & 36 &  \\ -->
<!--     \hline -->
<!--   \end{tabular} -->
<!-- \end{center} -->
<!-- The sensitivity is then given by  -->
<!-- $$\frac{\text{#True Positive}}{\text{#Condition Positive}}=\frac{24}{30}=0.8$$, while the specificity is given by  -->
<!-- $$\frac{\text{#True Negative}}{\text{#Condition Negative}}=\frac{30}{35}=0.86$$.  -->
<!-- -->

> lda_train=lda(y~x1+x2, data=train)
> lda_train
Call:
lda(y ~ x1 + x2, data = train)

Prior probabilities of groups:
        0         1 
0.4461538 0.5538462 

Group means:
        x1       x2
0 17.25517 5.577241
1 20.17500 2.966944

Coefficients of linear discriminants:
           LD1
x1  0.06067097
x2 -0.97009878
> lpred=predict(lda_train, newdata=test)
> lpred
$class
 [1] 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1
[29] 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1
[57] 0 0 1 0 1 1 1 1 1
Levels: 0 1

$posterior
               0            1
99  0.6030427855 3.969572e-01
123 0.0007043195 9.992957e-01
42  0.4478408148 5.521592e-01
65  0.0232478366 9.767522e-01
26  0.0444283977 9.555716e-01
115 0.0116035186 9.883965e-01
124 0.0062499760 9.937500e-01
8   0.8820213252 1.179787e-01
107 0.0720745989 9.279254e-01
127 0.1755376042 8.244624e-01
63  0.2075896847 7.924103e-01
46  0.9086621879 9.133781e-02
128 0.0056624102 9.943376e-01
83  0.0014551149 9.985449e-01
116 0.0009979845 9.990020e-01
60  0.0067892945 9.932107e-01
50  0.9999947844 5.215588e-06
120 0.0002952431 9.997048e-01
47  0.8676646264 1.323354e-01
119 0.1128349577 8.871650e-01
86  0.0116468443 9.883532e-01
15  0.9999149156 8.508442e-05
75  0.0328181387 9.671819e-01
14  0.9811240725 1.887593e-02
121 0.0425379957 9.574620e-01
45  0.8893439692 1.106560e-01
54  0.9954104753 4.589525e-03
25  0.0828510389 9.171490e-01
30  0.7949115928 2.050884e-01
129 0.0010865338 9.989135e-01
122 0.9380173669 6.198263e-02
52  0.9713372189 2.866278e-02
61  0.0828838199 9.171162e-01
84  0.6339377450 3.660623e-01
19  0.9999923906 7.609383e-06
82  0.2300226860 7.699773e-01
27  0.8321845893 1.678154e-01
109 0.0121870916 9.878129e-01
1   0.9799882315 2.001177e-02
56  0.9908884780 9.111522e-03
101 0.0709942992 9.290057e-01
39  0.2330086600 7.669913e-01
40  0.9461607873 5.383921e-02
73  0.0770224224 9.229776e-01
21  0.9791993538 2.080065e-02
24  0.2758882001 7.241118e-01
43  0.9688679083 3.113209e-02
13  0.9763470942 2.365291e-02
111 0.0188607621 9.811392e-01
31  0.9485678450 5.143216e-02
10  0.9996572987 3.427013e-04
106 0.0074781706 9.925218e-01
9   0.9524720686 4.752793e-02
59  0.9988413282 1.158672e-03
2   0.7862957170 2.137043e-01
76  0.2668325819 7.331674e-01
62  0.9817125580 1.828744e-02
68  0.6705002227 3.294998e-01
102 0.0068951342 9.931049e-01
58  0.9904363777 9.563622e-03
5   0.3855864382 6.144136e-01
93  0.0228736234 9.771264e-01
89  0.0103571181 9.896429e-01
96  0.0093967145 9.906033e-01
103 0.0114184444 9.885816e-01

$x
            LD1
99  -0.38003236
123  2.45297448
42  -0.14840934
65   1.15395624
26   0.90681985
115  1.41480958
124  1.64516825
8   -0.96819057
107  0.71741179
127  0.34523588
63   0.26870130
46  -1.07363707
128  1.68182554
83   2.18488519
116  2.32423739
60   1.61441885
50  -4.71520507
120  2.77402121
47  -0.91974931
119  0.53539887
86   1.41341786
15  -3.68469003
75   1.02307250
14  -1.68388517
121  0.92359758
45  -0.99489216
54  -2.21114953
25   0.66167091
30  -0.72572955
129  2.29282858
122 -1.22847080
52  -1.52601329
61   0.66151171
84  -0.42837810
19  -4.57578919
82   0.22022820
27  -0.81667233
109  1.39648093
1   -1.66189079
56  -1.95636328
101  0.72341521
39   0.21403375
40  -1.28364779
73   0.69093307
21  -1.64732339
24   0.13045426
43  -1.49457263
13  -1.59881845
111  1.23279666
31  -1.30146701
10  -3.17037848
106  1.57849385
9   -1.33212089
59  -2.72046731
2   -0.70651861
76   0.14735935
62  -1.69579649
68  -0.48791207
102  1.60867014
58  -1.93832119
5   -0.05373715
93   1.16008702
89   1.45721559
96   1.49349081
103  1.42081300

> table(test$y, lpred$class)
   
     0  1
  0 24  6
  1  5 30