---
title: "Prosjekt 1"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 2

```{r}
library(ggplot2)
data = read.table("https://www.math.ntnu.no/emner/TMA4268/2018v/data/SYSBPreg3uid.txt")
dim(data)
colnames(data)
modelA=lm(-1/sqrt(SYSBP) ~ .,data = data)
summary(modelA)
```


#a)
In this problem, we are going to model $-\frac{1}{\sqrt{SYSBP}}$ as a function of the covariates SEX, AGE, CURSMOKE, BMI, TOTCHOL and BPMEDS.
The equation for the fitted model A, where $-\frac{1}{\sqrt{SYSBP}}$ is the response, is 
$$\boldsymbol{Y}=\boldsymbol{\beta}\boldsymbol{X}+\boldsymbol{\epsilon},$$

where $\boldsymbol{{\beta}}$ is a vector consisting of the regression parameterers corresponding to each covariate, including the intercept, $\beta_0$.

In the summary output, the column with `Estimate` is the estimated values for the coefficients of the model, the $\hat{\beta}'s$. In particular, `Intercept` is the estimate for $\beta_0$. The other estimated coefficients tell us how much we can expect $-\frac{1}{\sqrt{SYSBP}}$ to change given a change i  the respective covariates. The parameters in $\boldsymbol{\beta}$ can be estimated with maximum likelihood and least squares, which both gives $$\boldsymbol{\hat{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}.$$

`Std. Error` is the standard error of the coeffiecients. These values tell us how close the estimated coefficients are to the true values of the coefficients.

Further, the values under `t value` tell us how many standard deviations $\beta_i$ is away from 0. These values are calculated by $$t_i = \frac{\hat{\beta_i}}{SE(\hat{\beta_i})}.$$
In other words, if the t-value is large, the variable is most likely related to the response. 

If there is no relation between the variable and the response, then the t-statistic is expected to have a t-distribution with n-2 degrees of freedom. Due to the central limit theorem, the t-distribution will be quite similar to the normal distribution for values of n larger than approximately 30. Thus, it is easy to compute the probability of observing any number larger than $|t|$. This probabilities are the p-values, and they are shown in the under `Pr(>|t|)` in the summary output. Small p-values tell us that it is very unlikely that the observed association between the variable and the response is only due to chance. Thus the p-values indicate whether each of the variables is related to the response.

The `Residual standard error` is an estimate of the standard deviation of the error term $\epsilon$. We have that $\sigma^2=Var(\epsilon)$, so the Residual standard error, RSE, is an estimate of $\sigma$. The formula for this is $RSE = \sqrt{RSS/(n-2)}$, where RSS is the residual sum of squares. The residual standard error is a measurement of how much the model deviates from the true data. 

In order to see whether the regression is significant,  there is a relationship between the response and the covariates at all, one can use a hypothesis test. If the null hypothesis is set to be H_0:  $\beta_{SEX} = \beta_{AGE} = ... = \beta_{BPMEDS} = 0$, with the alternative H_1: at least one $\beta_j$ is non-zero, the `F-statistic` can be computed. The formula of this is $$F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}.$$ If the F-statistric is close to 1, it may be an indication of an association between the respons and the predictors. Here, the F-statistic is much larger than 1, so we can expect the alternative hypothesis to be true. 

#b)
While the residual standard error gives an absolute measure of lack of fit of the model, the $R^2$-statistic provides a value between 0 and 1. This value is the proportion of variance which the the model can explain. Here, the $R^2$-value of the data is 0.2494. In other words, about 1/4 of the variability can be explained by the fitted `modelA`.

```{r,echo=FALSE,eval=TRUE}
ggplot(modelA, aes(.fitted, .resid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle = deparse(modelA$call))
# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelA$call))

```

Diagnostic plots of "fitted values vs. standardized residuals" and "QQ-plot of standardized residuals" for modelA are shown above. From these plots, we can see that the model seems to fit well. The plot of the residuals vs fitted values shows that the residuals mostly are equally spread around a horizontal line. This is a good indication of a linear relationship between the response and the covariates. In addition, the residuals in the qq-plot follow almost a straight line all the way. Thus, the residuals seem to be Gaussian distributed.

We then make a new model, modelB, where $SYSBP$ is the response.

```{r}
modelB=lm(SYSBP ~ .,data = data)
summary(modelB)
```

For this model, the plot of residuals vs fitted values and the qq-plot are as follows:

```{r,echo=FALSE}
# residuls vs fitted
ggplot(modelB, aes(.fitted, .resid)) + geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(x = "Fitted values", y = "Residuals", title = "Fitted values vs. residuals", subtitle = deparse(modelB$call))
# qq-plot of residuals
ggplot(modelB, aes(sample = .stdresid)) +
  stat_qq(pch = 19) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  labs(x = "Theoretical quantiles", y = "Standardized residuals", title = "Normal Q-Q", subtitle = deparse(modelB$call))
```

The plot of fitted values vs residuals for modelB seems quite similar to the one for modelA. However, the residuals in the qq-plot do not follow a straight line, so the residuals are probably not Gaussian distributed. In addition, we can do the Anderson-Darling normality test:

```{r}
library(nortest)
ad.test(rstudent(modelA))
ad.test(rstudent(modelB))
```

From these p-values, we can clearly conclude that the residuals in modelB is not Gaussian distributed, while the opposite is most likely true in modelA.

Due to these comparisons, I would prefer modelA if the aim is to make inference about systolic blood pressure. The transformation of the response to $-\frac{1}{\sqrt{SYSBP}}$ gives residuals which are akmost Gaussian distributed, so this one is preferable. 


#c)
Using modelA, we can from the estimate-column in the summary-output find the estimates for the coeffiecients, the $\hat{\beta}$'s, belonging to each covariate. Thus, we can see that $$\hat{\beta_{BMI}}=3.087\cdot10^{-4}.$$

Therefore, according to the model, if the BMI is increased by 1 while the other covariates are fixed, the response $-\frac{1}{\sqrt{SYSBP}}$ is estimated to increase by $3.087\cdot10^{-4}$. 

We have that for each $\beta_j$, $\hat{\beta}_j \sim N(\beta_j,\text{Var}(\hat{\beta}_j))$ and 
$$T_j =\frac{\hat{\beta}_j-\beta_j}{\sqrt{\widehat{\text{Var}}(\hat{\beta}_j)}} \sim t_{n-2}.$$
Using this, we can construct a confidence interval for $\hat{\beta_{BMI}}$. This interval will then be on the form $\hat{\beta_{BMI}} \pm t_{\alpha/2,n-2}SE(\hat{\beta_{BMI}})$. In order to make a 99 % confidence interval, we have $\alpha=0.01$. Thus, we can look up in a table to find the value $t_{0,005,n-2} \approx z_{0,005} = 2.58$. Since $n=2600 > 30$, we can use this approximation by the Central Limit Theorem. Now, by using the values of $\hat{\beta_{BMI}}$ and $SE(\hat{\beta_{BMI}})$ from the summary-output, we obtain the 99 % confidence interval $(3.087 \cdot 10^{-4} \pm 2.58 * 2.955 \cdot 10^{-5}),$ $$(2.325 \cdot 10^{-4}, 3.849 \cdot 10^{-4}).$$ 

The interpretation of this interval is that we can have 99 % confidence that the true $\beta$ is within this interval. Considering the hypothesis test $H_0: \beta_{BMI} = 0$ against $H_1: \beta_{BMI} \neq 0$, we can get information about the p-value for this test through the confidence interval. Since the confidence interval does not contain the null hypothesis value $\beta_{BMI}=0$, we now that the p-value is less than the significance level $\alpha = 0.01$. Equivalently, the hypothesis test is statistically significant.   

#d)
Now we consider a person with these data:
```{r}
names(data)
new=data.frame(SEX=1,AGE=56,CURSMOKE=1,BMI=89/1.75^2,TOTCHOL=200,BPMEDS=0)

```

In order to make a guess for his $-\frac{1}{\sqrt{SYSBP}}$, we insert these data into the the equation for the multiple linear regression modelA, with the estimated values $\hat{\beta_j}$ from task a. We then get the response value $-0.086$, which is our best guess. If we have that $y=-\frac{1}{\sqrt{SYSBP}}$, we have that the inverse function is equal to $SYSBP=\frac{1}{y^2}$. Thus, when using that the best guess for $y$ is $-0.08667$, we get that the best guess for the systolic blood pressure of this person is $SYSBP = \frac{1}{(-0.086)^2} = 133.1$.

To make a 90 % prediction interval for this persons systolic blood pressure SYSBP, we can construct a prediction interval for the response of modelA, and then transform the limits of the interval by the inverse function of $-\frac{1}{\sqrt{SYSBP}}$:
```{r}
predict(modelA,newdata=new,interval ="prediction",type="response",level=0.90)
"Limits of prediction interval for SYSBP:"
lower = 1/(predict(modelA,newdata=new,interval ="prediction",type="response",level=0.90)[2]^2)
lower
upper = 1/(predict(modelA,newdata=new,interval ="prediction",type="response",level=0.90)[3]^2)
upper
```

Thus, according to our model, the probability is 90 % that this person has a systolic blood pressure between 107.9 and 168.3. The range of this interval is very large, so the prediction interval is not that informative. If we look at the dataset, we can see that a very large amount of the people in the study has a systolic blood pressure in this interval. Therefore, we would have wanted a smaller range of the prediction interval in order to get useful information from it.  



