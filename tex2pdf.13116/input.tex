\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Compulsory exercise 1: Group X},
            pdfauthor={NN1, NN and NN3},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Compulsory exercise 1: Group X}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
\subtitle{TMA4268 Statistical Learning V2018}
  \author{NN1, NN and NN3}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{16.02.2018}


\begin{document}
\maketitle

\section{Problem 1 - Core concepts in statistical learning {[}2
points{]}}\label{problem-1---core-concepts-in-statistical-learning-2-points}

\subsection{a) Training and test MSE}\label{a-training-and-test-mse}

\begin{itemize}
\item
  We see that for K=1, the variance is pretty high, which is expected as
  the curves only will go from point to point. At K=2 we see that the
  variance lowers some as we now take into account the two nearest
  points, and the peaks will not be that far away from the true line.
  When K increases, we take account for neigbhours which is not
  necessary very local anymore. Which K to choose to keep the
  neighbourhood local depends on the number of observations in the
  training set. Here n=61, and already when K=10, we get somewhat of a
  problem in the end points. Although we get a very good fit along most
  of the line, at the end it will take the same points into account, and
  one will get a straight line. At K=61, one will only get a straight
  line, which is the mean.
\item
  A small value of K will give a more flexible fit, which has low bias
  but high variance. As mentioned above, K=1 will give point-to-point
  plots, which is an overfit. At K=2 we get a lower variance, and there
  is no sign of underfitting the curve. At K=10 the variance is notably
  lower, but the plot shows signs of underfitting and a low flexibility.
  Continuing the trend, at K=25 we get a clearly underfitted and
  inflexible result.
\item
  The MSE for the training sets show how close the predicted curve is to
  the points it is based on. As we can see, at K=1 the MSE is zero,
  which is a clear sign of overfitting. The MSE just increases as K
  increases, which indicates low flexibility. There is by this plot hard
  to say where the overfitting stops and the low flexibility starts, and
  thus hard to conclude what is the best value for K. On the other hand,
  the MSE for the test set decreases for the first K's, and then
  increases. The test set is not affected by overfitting in the same
  way, and will therefore show the best value for K much clearer.
\item
  The lowest point, being around K=5 here, is where the error is lowest
  and would be our choice for K.
\end{itemize}

\subsection{b) Bias-variance trade-off}\label{b-bias-variance-trade-off}

\begin{itemize}
\tightlist
\item
  You start by fitting a curve to the training data sets by the KNN
  method. Then you can express the fitted curve as
  \(Y = f(x) + \epsilon\). Here \(\epsilon\) is the random or
  irreducible error, and we assume it has mean zero and constant
  variance equal to \(\sigma^2\). The expected test mean squared error
  (MSE) for \(x_0\) is defined as: \[ E[Y- \hat f (x_0)]^2 .\] Then we
  decompose the MSE into three terms:
  \[E[(Y- \hat f (x_0))^2] = Var( \epsilon ) + Var[\hat f  (x_0)] + [Bias(\hat f(x_0))]^2 ,\]
  and
\item
  Interpretation of Figure 4:

  \begin{itemize}
  \tightlist
  \item
    The squared bias decreases as the flexibility increases. That means
    that for high flexibility there is no systematic difference from the
    predicted model and the values being estimated. At higher values for
    K there will be a systematic difference. As example, in a) when
    K=61, we will always get a straight line, which differs from the
    true values.
  \item
    The variance decreases as K increases, since more points are taken
    account for, and the difference from each training set to the others
    is less.
  \item
    The irreducible error is not dependent on the values of K, and is
    therefore constant.
  \end{itemize}
\item
  Based on the sum of the errors, the lowest error is for K=3 (but
  nearly the same as K=5). Thus this will be our choice for K as this
  will give the most accurate model. This agrees fairly well with what
  we found in a), and our conclusion would be that a K between 3 and 5
  would give the best results.
\end{itemize}

\section{Problem 2 - Linear
regression}\label{problem-2---linear-regression}

\subsection{a) Understanding model
output}\label{a-understanding-model-output}

\begin{itemize}
\tightlist
\item
  In this problem, we are going to model \(-\frac{1}{\sqrt{SYSBP}}\) as
  a function of the covariates SEX, AGE, CURSMOKE, BMI, TOTCHOL and
  BPMEDS, using the data from n = 2600 persons. The equation for the
  fitted \texttt{modelA}, where \(-\frac{1}{\sqrt{SYSBP}}\) is the
  response, is
  \[\boldsymbol{Y}=\boldsymbol{\beta}\boldsymbol{X}+\boldsymbol{\epsilon},\]
\end{itemize}

where \(\boldsymbol{\beta}\) is a vector consisting of the regression
parameters corresponding to each covariate, including the intercept,
\(\beta_0\), and \(\boldsymbol{\epsilon}\) is the error term, assumed to
be normally distributed with mean 0.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelA=}\KeywordTok{lm}\NormalTok{(}\OperatorTok{-}\DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(SYSBP) }\OperatorTok{~}\StringTok{ }\NormalTok{.,}\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{summary}\NormalTok{(modelA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = -1/sqrt(SYSBP) ~ ., data = data)
## 
## Residuals:
##        Min         1Q     Median         3Q        Max 
## -0.0207366 -0.0039157 -0.0000304  0.0038293  0.0189747 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -1.103e-01  1.383e-03 -79.745  < 2e-16 ***
## SEX         -2.989e-04  2.390e-04  -1.251 0.211176    
## AGE          2.378e-04  1.434e-05  16.586  < 2e-16 ***
## CURSMOKE    -2.504e-04  2.527e-04  -0.991 0.321723    
## BMI          3.087e-04  2.955e-05  10.447  < 2e-16 ***
## TOTCHOL      9.288e-06  2.602e-06   3.569 0.000365 ***
## BPMEDS       5.469e-03  3.265e-04  16.748  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.005819 on 2593 degrees of freedom
## Multiple R-squared:  0.2494, Adjusted R-squared:  0.2476 
## F-statistic: 143.6 on 6 and 2593 DF,  p-value: < 2.2e-16
\end{verbatim}

\begin{itemize}
\item
  In the summary output, the column with \texttt{Estimate} is the
  estimated values for the coefficients of the model, the
  \(\hat{\beta}'s\). In particular, \texttt{Intercept} is the estimate
  for \(\beta_0\). This is the value of the response when all the
  covariates are 0. In this case, this value is not that meaningful,
  since it for instance makes no sense to have a person with a BMI of 0.
  The other estimated coefficients tell us how much we can expect
  \(-\frac{1}{\sqrt{SYSBP}}\) to change given a change in the respective
  covariate when all the other covariates are fixed. The parameters in
  \(\boldsymbol{\beta}\) can be estimated with maximum likelihood and
  least squares, which both gives
  \[\boldsymbol{\hat{\beta}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}.\]
\item
  \texttt{Std.\ Error} is the standard error of the coefficients. These
  values tell us how close the estimated coefficients are to the true
  values of the coefficients. The standard error of each estimated
  coefficient is found by taking the square root of the corresponding
  term on the diagonal of the covariance matrix
  \(\Sigma = \sigma^2(\boldsymbol{X}^T\boldsymbol{X})\), where
  \(\sigma\) is the variance of the resiudals.
\item
  Further, the values under \texttt{t\ value} tell us how many standard
  deviations \(\beta_i\) is away from 0. For each covariate, we
  investigate whether there is a relationship between the covariate and
  the response. This is done through the hypothesis test
  \(H_0: \beta_i = 0\) versus \(H_1: \beta_i \neq 0\). Then the
  t-statistic is calculated by
  \[t_i = \frac{\hat{\beta_i}-0}{SE(\hat{\beta_i})}.\] This is the
  formula for the t-value of each covariate. We can see that if the
  t-value is large, the variable is most likely related to the response.
\item
  If there is no relation between the variable and the response, then
  the t-statistic is expected to have a t-distribution with n-2 degrees
  of freedom. Due to the Central Limit Theorem, the t-distribution will
  be quite similar to the normal distribution for values of n larger
  than approximately 30. Thus, it is easy to compute the probability of
  observing any number larger than \(|t|\). These probabilities are the
  p-values, and they are shown in the column under
  \texttt{Pr(\textgreater{}\textbar{}t\textbar{})} in the summary
  output. Small p-values tell us that it is very unlikely that the
  observed association between the variable and the response is only due
  to chance. Thus the p-values indicate whether each of the variables is
  related to the response, so we can find out whether the hypothesis
  test described above is significant or not.
\item
  The \texttt{Residual\ standard\ error} is an estimate of the standard
  deviation of the error term \(\epsilon\). We have that
  \(\sigma^2=Var(\epsilon)\), so the Residual standard error, RSE, is an
  estimate of \(\sigma\). The formula for this is
  \(RSE = \sqrt{RSS/(n-2)}\), where RSS is the residual sum of squares,
  \(RSS = \sum_{i=1}^{n}(y_i-\hat{y_i})\). The residual standard error
  is an absolute measurement of how much the model deviates from the
  true data.
\item
  In order to see whether the regression is significant, that means
  whether there is a relationship between the response and the
  covariates at all, one can use another hypothesis test. If the null
  hypothesis is set to be
  \(H_0: \beta_{SEX} = \beta_{AGE} = ... = \beta_{BPMEDS} = 0\), with
  the alternative \(H_1:\) at least one \(\beta_j\) is non-zero, the
  \texttt{F-statistic} can be computed. The formula for this is
  \[F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)},\] where
  \(TSS=\sum_{i=1}^{n}(y_i-\hat{y_i})\). If the F-statistic is close to
  1, it may be an indication of an association between the response and
  the predictors. Here, the F-statistic is much larger than 1, so we can
  expect the alternative hypothesis to be true.
\end{itemize}

\subsection{b) Model fit}\label{b-model-fit}

\begin{itemize}
\tightlist
\item
  While the residual standard error gives an absolute measure of lack of
  fit of the model, the \(R^2\)-statistic provides a value between 0 and
  1. To calculate this, we use the formula \(R^2 = 1-\frac{RSS}{TSS}\).
  This value is the proportion of variance which the model can explain.
  We can say that TSS is a measurement of the total variance in the
  response \(Y\), while RSS is the amount of variability that the
  regression model does not explain. Thus, the \(R^2\)-value becomes the
  proportion of variance which the model is able to explain. Here, the
  \(R^2\)-value of the data is 0.2494. In other words, about 1/4 of the
  variability can be explained by the fitted \texttt{modelA}.
\end{itemize}

\includegraphics{Project1_files/figure-latex/unnamed-chunk-3-1.pdf}
\includegraphics{Project1_files/figure-latex/unnamed-chunk-3-2.pdf}

\begin{itemize}
\item
  Diagnostic plots of fitted values against standardized residuals and
  QQ-plot of standardized residuals for modelA are shown above. From
  these plots, we can see that the model seems to fit well. The plot of
  the residuals vs fitted values shows that the residuals, except for
  the largest fitted values, are equally spread around a horizontal
  line. This is a good indication of a linear relationship between the
  response and the covariates. In addition, the residuals in the qq-plot
  follow almost a straight line all the way. Thus, the residuals seem to
  be Gaussian distributed.
\item
  We then make a new model, modelB, where SYSBP is the response:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{modelB=}\KeywordTok{lm}\NormalTok{(SYSBP }\OperatorTok{~}\StringTok{ }\NormalTok{.,}\DataTypeTok{data =}\NormalTok{ data)}
\KeywordTok{summary}\NormalTok{(modelB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = SYSBP ~ ., data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -59.800 -13.471  -1.982  11.063  88.959 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 56.505170   4.668798  12.103  < 2e-16 ***
## SEX         -0.429973   0.807048  -0.533  0.59424    
## AGE          0.795810   0.048413  16.438  < 2e-16 ***
## CURSMOKE    -0.518742   0.853190  -0.608  0.54324    
## BMI          1.010550   0.099770  10.129  < 2e-16 ***
## TOTCHOL      0.028786   0.008787   3.276  0.00107 ** 
## BPMEDS      19.203706   1.102547  17.418  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 19.65 on 2593 degrees of freedom
## Multiple R-squared:  0.2508, Adjusted R-squared:  0.249 
## F-statistic: 144.6 on 6 and 2593 DF,  p-value: < 2.2e-16
\end{verbatim}

For this model, the plot of residuals against fitted values and the
qq-plot are as follows:

\includegraphics{Project1_files/figure-latex/unnamed-chunk-5-1.pdf}
\includegraphics{Project1_files/figure-latex/unnamed-chunk-5-2.pdf}

The plot of fitted values vs residuals for modelB seems quite similar to
the one for modelA. However, although the residuals mostly are spread
around a horizontal line, it is seemingly not the case for all the
largest and smalles fitted values. It is however difficult to state
whether this indicates that the assumption of linear relationship
between the response and the covariates does not hold. Also, the
residuals in the qq-plot do not quite follow a straight line, so the
residuals are probably not Gaussian distributed.

In addition, we can do the Anderson-Darling normality test for the two
models:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(nortest)}
\KeywordTok{ad.test}\NormalTok{(}\KeywordTok{rstudent}\NormalTok{(modelA))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Anderson-Darling normality test
## 
## data:  rstudent(modelA)
## A = 0.19209, p-value = 0.8959
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ad.test}\NormalTok{(}\KeywordTok{rstudent}\NormalTok{(modelB))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Anderson-Darling normality test
## 
## data:  rstudent(modelB)
## A = 13.2, p-value < 2.2e-16
\end{verbatim}

This test tells us how well the data follow the Gaussian distribution.
The better the distribution fits the data, the smaller the p-value will
be. From the p-values here, we can clearly conclude that the residuals
in modelB are not Gaussian distributed, while the opposite is most
likely true in modelA.

Due to these comparisons, we would prefer modelA if the aim is to make
inference about systolic blood pressure. Unlike the residuals in modelB,
the transformation of the response to \(-\frac{1}{\sqrt{SYSBP}}\) gives
residuals which are almost Gaussian distributed, so this one is
preferable.

\subsection{c) Confidence interval and hypothesis
test}\label{c-confidence-interval-and-hypothesis-test}

\begin{itemize}
\item
  Using modelA, we can from the estimate-column in the summary-output
  find the estimates for the coeffiecients, the \(\hat{\beta}\)'s,
  belonging to each covariate. Thus, we can see that
  \[\hat{\beta}_{BMI}=3.087\cdot10^{-4}.\]
\item
  Therefore, according to the model, if the BMI is increased by 1 while
  the other covariates are fixed, the response
  \(-\frac{1}{\sqrt{SYSBP}}\) is estimated to increase by
  \(3.087\cdot10^{-4}\).
\item
  We have that for each \(\beta_j\),
  \(\hat{\beta}_j \sim N(\beta_j,\text{Var}(\hat{\beta}_j))\) and
  \[T_j =\frac{\hat{\beta}_j-\beta_j}{\sqrt{\widehat{\text{Var}}(\hat{\beta}_j)}} \sim t_{n-2}.\]
  Using this, we can construct a confidence interval for
  \(\hat{\beta}_{BMI}\). We have that
  \[P(-t_{\alpha/2,n-2} < \frac{\hat{\beta}_j-\beta_j}{\sqrt{\widehat{\text{Var}}(\hat{\beta}_j)}} < t_{\alpha/2,n-2}) = 1 - \alpha).\]
  From this, we will get an interval on the form
  \(\hat{\beta}_{BMI} \pm t_{\alpha/2,n-2}SE(\hat{\beta}_{BMI})\). In
  order to make a 99 \% confidence interval, we have \(\alpha=0.01\).
  Thus, we can look up in a table to find the value
  \(t_{0,005,n-2} \approx z_{0,005} = 2.58\). Since \(n=2600 > 30\), we
  can use this approximation by the Central Limit Theorem. Now, by using
  the values of \(\hat{\beta}_{BMI}\) and \(SE(\hat{\beta}_{BMI})\) from
  the summary-output, we obtain the 99 \% confidence interval
  \((3.087 \cdot 10^{-4} \pm 2.58 * 2.955 \cdot 10^{-5}),\)
  \[(2.325 \cdot 10^{-4}, 3.849 \cdot 10^{-4}).\]
\item
  The interpretation of this interval is that we can have 99 \%
  confidence that the true \(\beta\) is within this interval.
  Considering the hypothesis test \(H_0: \beta_{BMI} = 0\) against
  \(H_1: \beta_{BMI} \neq 0\), we can get information about the p-value
  for this test through the confidence interval. Since the confidence
  interval does not contain the null hypothesis value \(\beta_{BMI}=0\),
  we know that the p-value is less than the significance level
  \(\alpha = 0.01\). Equivalently, the hypothesis test is statistically
  significant.
\end{itemize}

\subsection{d) Prediction}\label{d-prediction}

Now we consider a person with these data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "SYSBP"    "SEX"      "AGE"      "CURSMOKE" "BMI"      "TOTCHOL" 
## [7] "BPMEDS"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new=}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{SEX=}\DecValTok{1}\NormalTok{,}\DataTypeTok{AGE=}\DecValTok{56}\NormalTok{,}\DataTypeTok{CURSMOKE=}\DecValTok{1}\NormalTok{,}\DataTypeTok{BMI=}\DecValTok{89}\OperatorTok{/}\FloatTok{1.75}\OperatorTok{^}\DecValTok{2}\NormalTok{,}\DataTypeTok{TOTCHOL=}\DecValTok{200}\NormalTok{,}\DataTypeTok{BPMEDS=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\item
  In order to make a guess for his \(-\frac{1}{\sqrt{SYSBP}}\), we
  insert these data into the equation for the multiple linear regression
  modelA, with the estimated values \(\hat{\beta_j}\) from task a. We
  then get the response value \(-0.08667\), which is our best guess. If
  we have that \(y=-\frac{1}{\sqrt{SYSBP}}\), we have that the inverse
  function is equal to \(SYSBP=\frac{1}{y^2}\). Thus, when using that
  the best guess for \(y\) is \(-0.08667\), we get that the best guess
  for the systolic blood pressure of this person is
  \(SYSBP = \frac{1}{(-0.08667)^2} = 133.1\).
\item
  To make a 90 \% prediction interval for this person's systolic blood
  pressure SYSBP, we can construct a prediction interval for the
  response of \texttt{modelA} around our best guess, and then transform
  the limits of the interval by the inverse function of
  \(-\frac{1}{\sqrt{SYSBP}}\):
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(modelA,}\DataTypeTok{newdata=}\NormalTok{new,}\DataTypeTok{interval =}\StringTok{"prediction"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"response"}\NormalTok{,}\DataTypeTok{level=}\FloatTok{0.90}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           fit         lwr         upr
## 1 -0.08667246 -0.09625664 -0.07708829
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"Limits of prediction interval for SYSBP:"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Limits of prediction interval for SYSBP:"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lower =}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(}\KeywordTok{predict}\NormalTok{(modelA,}\DataTypeTok{newdata=}\NormalTok{new,}\DataTypeTok{interval =}\StringTok{"prediction"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"response"}\NormalTok{,}\DataTypeTok{level=}\FloatTok{0.90}\NormalTok{)[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{lower}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 107.9291
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upper =}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(}\KeywordTok{predict}\NormalTok{(modelA,}\DataTypeTok{newdata=}\NormalTok{new,}\DataTypeTok{interval =}\StringTok{"prediction"}\NormalTok{,}\DataTypeTok{type=}\StringTok{"response"}\NormalTok{,}\DataTypeTok{level=}\FloatTok{0.90}\NormalTok{)[}\DecValTok{3}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\NormalTok{upper}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 168.2764
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Thus, according to our model, the probability is 90 \% that this
  person has a systolic blood pressure between 107.9 and 168.3. The
  range of this interval is very large, so the prediction interval is
  not that informative. If we look at the dataset, we can see that a
  very large amount of the people in the study has a systolic blood
  pressure in this interval. Therefore, we would have wanted a smaller
  range of the prediction interval in order to get useful information
  from it.
\end{itemize}

\section{Problem 3 - Classification}\label{problem-3---classification}

\subsection{a) Logistic regression}\label{a-logistic-regression}

\begin{itemize}
\tightlist
\item
  We want to show that \(logit(p_i) = log(\frac{p_i}{1-p_i})\) is a
  linear function, where
  \(p_i = \frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}\).
  Thus, we get: \[
  \begin{split}
  log \Big(\frac{p_i}{1-p_i}\Big) &= log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{1-\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg)= log \Bigg(\frac{\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}}{\frac{1}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2})}} \Bigg) \\ 
  &= log \Bigg(exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}) \Bigg) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}.
  \end{split}
  \] And we get
  \(log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}\),
  which is a linear function.
\end{itemize}

\begin{verbatim}
## (Intercept)          x1          x2 
##   7.3143074   0.1332006  -2.3360758
\end{verbatim}

\begin{itemize}
\item
  There is no obivous interpretation of the \(\hat{\beta_1}\) and
  \(\hat{\beta_2}\) since an increase of one unit in \(\hat{\beta}\)
  does not give a consistent increase or decrease for all values of
  \(x_1\). It is however easier to look at the odds
  \(\frac{p_i}{1-p_i}\), which is bounded from below, but not above. The
  interpretation of the odds in this example is the conditional
  probability that a wine is in class 1, divided by the probability that
  the wine is in class 0 for the covariates \(x_1\) and \(x_2\).
  Moreover, if there is an increase from \(x_{1i}\) to \(x_{1i} + 1\),
  the odds are multiplied by \(e^{\beta_1}\), which gives us a more
  intuitive interpretation of \(\hat{\beta_1}\). The case is similar for
  \(\hat{\beta_2}\) for an increase in \(x_{2i}\), and we can see that
  for \(\beta < 0\) the odds will decrease, for \(\beta_1 = 0\) the odds
  will stay the same, and for \(\beta_1 >0\), the odds will increase.
\item
  Since the odds is linear (as shown before), we have a linear class
  boundary. From earlier, we have:
  \(log \bigg(\frac{Pr(Y_i=1|X=x_i)}{Pr(Y_i=0|X=x_i)} \bigg) = log \Big(\frac{p_i}{1-p_i}\Big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}\),
  where we want to fit the coefficients by maximizing the likelihood.
  Then, where \(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} = 0\), we have
  the class boundary.
\end{itemize}

\includegraphics{Project1_files/figure-latex/unnamed-chunk-10-1.pdf}

\begin{verbatim}
## 
## Call:
## glm(formula = y ~ ., family = "binomial", data = train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.46217  -0.17536   0.09309   0.28590   2.49572  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)   7.3143     5.3382   1.370 0.170626    
## x1            0.1332     0.2194   0.607 0.543800    
## x2           -2.3361     0.6472  -3.609 0.000307 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 89.354  on 64  degrees of freedom
## Residual deviance: 30.027  on 62  degrees of freedom
## AIC: 36.027
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{itemize}
\item
  Using the summary output, we can get the coefficients \(\beta_0\),
  \(\beta_1\) and \(\beta_2\). Thus by using the formula: \[
  \begin{split}
  P(Y=1|x_1 = 17, x_2=3) &= \frac{exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} = \frac{exp( 7.3143 + 0.1332*17 - 2.3361*3)}{1+exp( 7.3143 + 0.1332*17 - 2.3361*3)} \\ 
  & = \frac{exp(2.5704)}{1+exp(2.5704)}=0.9289.
  \end{split}
  \] This result can be interpreted as the probability for a observation
  to be in class \(Y=1\) given the variable values \(x_1 = 17\) and
  \(x_2=3\).
\item
  The sensitivity is the proportion of correctly classified positive
  observations, and the specificity is the proportion of correctly
  classified negative observations. Thus, the values are between 0 and
  1, and the goal is to obtain high values.
\end{itemize}

\begin{longtable}[]{@{}llll@{}}
\toprule
& Predicted - & Predicted + & Total\tabularnewline
True - & 25 & 5 & 30\tabularnewline
True + & 5 & 30 & 35\tabularnewline
Total & 30 & 35 &\tabularnewline
\bottomrule
\end{longtable}

\begin{itemize}
\tightlist
\item
  The tables above shows the confusion table for the test data, based on
  the from the training data. It is important to note that a large value
  for only one of them is not necessarily any good. For example, a
  method which just puts every observation in class 1 will have
  sensitivity 1, but specificity 0. Therefore, we want high values
  (close to one), but balanced as well. There are cases where it is more
  important that we get the classification of one class right than the
  other, for which we can sacrifice the balance to obtain either a high
  sensitivity or a high specificity (example would be to avoid a Type 1
  error, for which we can sacrifice the probability for actually
  rejecting a false \(H_0\)). That being said, in this case there is no
  worse to classify a 1-wine as a 0-wine, than the opposite.
\end{itemize}

\begin{verbatim}
## [1] 0.8571429
\end{verbatim}

\begin{verbatim}
## [1] 0.8333333
\end{verbatim}

They both obtain relativly large values, and one can therefor argue that
the classification works pretty good. Similar values for the sensitivity
and the spesificity suggests that the classification does not prefer or
default to one class, which in this case as discussed above is
desirable.

\subsection{b) K-nearest neighbor
classifier}\label{b-k-nearest-neighbor-classifier}

\[
Pr(Y=j|X=x_0)=\frac{1}{K} \sum_{i \in N_o} I(y_i=j)
\] * Given the point \(x_0\), the equation finds the K nearest points in
the dataset, and count the most frequent class among the neigbhours. The
right side of the equation finds the precentage of the neighbours which
belong to class j, and based on that, we take a decision whether \(x_=\)
should be in class j or not.

\begin{verbatim}
##    testclass3
##      0  1
##   0 25  5
##   1  3 32
\end{verbatim}

\begin{verbatim}
## [1] 0.9142857
\end{verbatim}

\begin{verbatim}
## [1] 0.8333333
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Here, the sensitivity is a little higher than the logistic regression,
  and the specificity stays the same. Thus, this method (with K=3) gets
  a total higher number of wines classified from the right cultivar.
  This is a good indicator that KNN with K=3 performs better in this
  example than the logistic regression.
\end{itemize}

\begin{verbatim}
##    testclass9
##      0  1
##   0 25  5
##   1  5 30
\end{verbatim}

\begin{verbatim}
## [1] 0.8571429
\end{verbatim}

\begin{verbatim}
## [1] 0.8333333
\end{verbatim}

\begin{itemize}
\tightlist
\item
  As we can see from the data, the sensitivity for K=3 is higher than
  for K=9, and the specificity is equal. That happens because the
  neighbours in k=9 becomes further away, and even though the formula
  take in account for more points, the points isn't as local anymore.
  Thus the bias increases, and the misclassification increases as well.
  The best value for K really depends on the number of points and how
  they are distributed, but as shown in 3d), one can calculate the ROC
  curve, and find the optimal K. Since there is a bias-variance
  trade-off, where low values for K will give high variance and high
  values for K will give a high bias, the optimal point will be where
  they togheter give the lowest mean squared error (MSE).
\end{itemize}

\subsection{c) LDA (\& QDA)}\label{c-lda-qda}

The expression from the task is found by assuming the classes are normal
distributed according to the function

\[f_k(x)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2}}e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_k})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_k})}.\]
The posterior probability is the calculated as
\[p_k(x)=Pr(Y=k|\boldsymbol{X}=\boldsymbol{x})=\frac{Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)Pr(Y=k)}{Pr(\boldsymbol{X}=\boldsymbol{x})}=\frac{\pi_kf_k(\boldsymbol{x})}{\sum_{l=1}^K\pi_lf_l(\boldsymbol{x})}\]

\begin{itemize}
\item
  Here, \(\pi_k=Pr(Y=k)\), which is the probability that a given wine
  sample \(Y\) belongs to class \(k\). \(\boldsymbol{\mu_k}\) is the
  vector containing the expected values of the parametres, which is
  color intensity and alcalinity of ash in this task.
  \(\boldsymbol{\Sigma}\) is the covariance matrix for the parametres,
  which contains the variance of the parametres and the covariance
  between them. \(f_k(x)\) is as mentioned the probability density
  function for a given class, which again is the conditional probability
  of \(x\) with a given class,
  \(f_k(x)=Pr(\boldsymbol{X}=\boldsymbol{x}|Y=k)\).
\item
  To estimate \(\pi_k\) we can use the the total number of samples in
  the training set \(n\), and the number of samples belonging to the
  requested class, \(n_k\). We can then use te estimator
  \(\hat{\pi}_k=\frac{n_k}{n}\). \(\mu_k\) can be estimated simply using
  a sample mean from all the samples belonging to the requested class.
  The estimator is then calculated as
  \(\hat{\mu}_k=\frac{1}{n_k}\sum_{i:y_i=k}x_i\). To estimate the
  covariance matrix, the class-specific covariance matrices can be
  calculated as
  \[\hat{\boldsymbol{\Sigma}}_k=\frac{1}{n_k-1}\sum_{i:y_i=k}(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)(\boldsymbol{x}_i-\bar{\boldsymbol{x}}_i)^T\]
  The true covariance matrix is the found by summing over the difference
  classes using wheighs according to the fraction of samples in the
  belonging class. We then get
  \[\hat{\boldsymbol{\Sigma}}=\sum_{k=1}^K \frac{n_k-1}{n-K}\cdot \hat{\boldsymbol{\Sigma}}_k.\]
  Calculations for the training data is done in R, resulting in values
  \(\pi_0=0.45\), \(\pi_1=0.55\),
  \(\boldsymbol{\mu_0}=(17.26 , 5.58)^T\),
  \(\boldsymbol{\mu_1}=(20.18, 2.97)^T\) and \[
  \hat{\boldsymbol{\Sigma}}=
  \left(\begin{array}{cc}
  7.23 & -0.66\\
  -0.66 & 0.95
  \end{array}\right)
  \]
\item
  We want to find when
  \[Pr(Y=0|\boldsymbol{X}=\boldsymbol{x})=Pr(Y=1|\boldsymbol{X}=\boldsymbol{x})\]
  We rewrite this as
  \[\frac{\pi_0 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_l(\boldsymbol{x})}=\frac{\pi_1 e^{-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1})}}{(2\pi)^{p/2}|\boldsymbol{\Sigma}|^{1/2} \sum_{l=1}^K \pi_l f_l(\boldsymbol{x})}\]
  Since the denominators are equal on both sides, we can disregard them.
  By taking logarithms on both sides we achieve
  \[-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_0})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_0}) + \log{\pi_0}=-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu_1})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu_1}) + \log{\pi_1}\]
  We now complete the miltiplications, and remove the equal parts on
  each side to get:
  \[\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} +\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}) + \log{\pi_0}=\frac{1}{2}(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} +\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x} - \boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}) + \log{\pi_1}\]
  Now we know the covariance matrix is symmetric positive definite. We
  therefore know
  \(\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} =\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{x}\).
  We therefore finally get
  \[\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x}),\] where
  \[\delta_k(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k}- \frac{1}{2}\boldsymbol{\mu_k}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_k} + \log{\pi_k}; \quad k\in \{0, 1\}\].
\end{itemize}

\begin{itemize}
\item
  To find the class boundary formula, we need to solve the equation
  \(\delta_0(\boldsymbol{x})=\delta_1(\boldsymbol{x})\). We get
  \[\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}- \frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} + \log{\pi_0}=\boldsymbol{x}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}- \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\pi_1}\]
  \[\boldsymbol{x}^T(\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0} - \frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1} + \log{\frac{\pi_1}{\pi_0}}\]
  We use estimators for the parametres as discussed earlier in this
  task. The necessary values are calculated in R, and we get
  \(\log{\frac{\pi_1}{\pi_0}}=0.216\),
  \(\frac{1}{2}\boldsymbol{\mu_1}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1}=-41.1\),
  \(\frac{1}{2}\boldsymbol{\mu_0}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}=-49.2\)
  and
  \((\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_0}-\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu_1})=(-0.17, 2.63)^T\).
  We then get the equation
  \[2.628x_2-0.164x_1=8.368 \implies \\ x_2 = 0.0625x_1 + 3.184\]
\item
  The following plot shows the training data as circular dots, together
  with the test data as crosses. The plot also shows the line
  representing equal probability of being in the two classes, based on
  the training data using LDA.
\item
  LDA is also performed using the built in R-function. This is done in
  the R-code shown below. We see the data corresponds to the estimators
  we have used earlier, which is natural as the estimators are
  calculated the same way.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda_train=}\KeywordTok{lda}\NormalTok{(y}\OperatorTok{~}\NormalTok{x1}\OperatorTok{+}\NormalTok{x2, }\DataTypeTok{data=}\NormalTok{train)}
\NormalTok{lda_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
## lda(y ~ x1 + x2, data = train)
## 
## Prior probabilities of groups:
##         0         1 
## 0.4461538 0.5538462 
## 
## Group means:
##         x1       x2
## 0 17.25517 5.577241
## 1 20.17500 2.966944
## 
## Coefficients of linear discriminants:
##            LD1
## x1  0.06067097
## x2 -0.97009878
\end{verbatim}

\newpage

\begin{longtable}[]{@{}llll@{}}
\toprule
& Predicted - & Predicted + & Total\tabularnewline
True - & 24 & 6 & 30\tabularnewline
True + & 5 & 30 & 35\tabularnewline
Total & 29 & 36 &\tabularnewline
\bottomrule
\end{longtable}

\begin{itemize}
\item
  The sensitivity is given by
  \[\frac{\text{True Positive}}{\text{Condition Positive}}=\frac{24}{30}=0.8,\]
  while the specificity is given by
  \[\frac{\text{True Negative}}{\text{Condition Negative}}=\frac{30}{35}=0.86.\]
  We want high specifity and seinsitivity, and we see values are fairly
  close to 1, which mean we get many correct predictions. It still seems
  like we will do mistakes every now and then, as there are a
  considerable number of mistakes.
\item
  In QDA, the covariance matrices for the different classes are allowed
  to be different. This makes this method more complex, but also more
  flexible. The decision boundaries will now become quadratic functions
  of \(\boldsymbol{x}\).
\end{itemize}

\subsection{d) Compare classifiers}\label{d-compare-classifiers}

\begin{itemize}
\item
  For logistic regression we got specificity 0.833 and sensitivity
  0.857. For KNN with our preferred \(K=3\), we got specificity 0.964
  and sensitivity 0.919. For lda we got 0.800 and specificity 0.857.
  Since we want high specificity and sensitivity, our preferred method
  when using 0.5 as cut-off is KNN. This method performs considerably
  better than the other two methods, which are more equal, though
  logistic regression seems to be slightly better. ha med dette?:Note
  however that the training and test sets are not that big, and that
  might influence which method performs better. Our result is still an
  indication that a strictly linear model might not be optimal for the
  wine classification problem.
\item
  The following three plots show ROC curves for specificity and
  sensitivity, and also prints the area under the curves. From the last
  point we concluded that KNN was the best method for cut-off equal to
  0.5. We do however see that the area under the curve is smaller for
  KNN than for the other methods. We also see on the plot the reason for
  this, assensitivity when the cut-off is low or high, and compared to
  the other methods, in the high or low cut-off areas the
  specificity/sensitivity is lower. It therefore seems like the other
  methods would work better for high or low cut-off, compared to KNN.
  Logistic regression and lda performs quite similar, which is natural
  as they make the both linearity-assumption, and the solutions are
  quite similar. the line in the lower left corner is less vertical, and
  the line in the upper right corner is less horizontal. This represents
  the specificity vs
\end{itemize}

\includegraphics{Project1_files/figure-latex/unnamed-chunk-19-1.pdf}

\begin{verbatim}
## Area under the curve: 0.9333
\end{verbatim}

\includegraphics{Project1_files/figure-latex/unnamed-chunk-19-2.pdf}

\begin{verbatim}
## Area under the curve: 0.9086
\end{verbatim}

\includegraphics{Project1_files/figure-latex/unnamed-chunk-19-3.pdf}

\begin{verbatim}
## function (...) 
## {
##     UseMethod("auc")
## }
## <environment: namespace:pROC>
\end{verbatim}


\end{document}
